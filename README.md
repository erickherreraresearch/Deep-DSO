# Deep-DSO
## Deep-DSO:  Improving Mapping of Direct Sparse Odometry Using CNN-based Single Image Depth Estimation
In the domain of computer vision, the challenges of three-dimensional reconstruction and self-motion computation have been extensively tackled through methodologies such as Simultaneous Localization and Mapping (SLAM), Visual Odometry (VO), and Structure from Motion (SfM). Among the diverse data sources applicable to these inherently complex problems, the utilization of a solitary monocular RGB camera has garnered considerable interest from the research community due to its cost-effectiveness and widespread presence in portable electronic devices. A prominent solution in this field is the Direct Sparse Odometry (DSO) framework, which has been proven to proficiently deduce precise trajectories and depth perceptions utilizing only monocular sequences.

In the present study, acknowledging the remarkable strides in single-image depth prediction via neural networks, we introduce an enhancement to the DSO framework, termed DeepDSO. This enhancement incorporates the cutting-edge NeW CRFs convolutional neural network (CNN) as a module for depth estimation. This module supplies preliminary depth information for each point of interest, thereby narrowing the search range along the epipolar line. Such integration augments the initialization process of depth points within the DSO algorithm, facilitating a more rapid convergence of each point towards its actual depth.

Empirical evaluations conducted on the TUM-Mono dataset have validated that the integration of the CNN-based depth estimation module within the DSO framework markedly diminishes errors related to rotation, translation, scale, and alignment at both the commencement and conclusion of segments, in addition to reducing the Root Mean Square Error (RMSE). The source code for DeepDSO has been made accessible to the public by this repository, fostering further research and development in this field.
